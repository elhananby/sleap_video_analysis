{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from scipy.stats import circmean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "from extract_stimulus_heading_for_camera import process_braidz_file, create_interpolation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN2HEADING_DATA = \"\"\"\n",
    "screen,heading\n",
    "0,2.3513283485530456\n",
    "80,1.2179812647799937\n",
    "160,0.5031545295746856\n",
    "240,-0.3078141744904855\n",
    "320,-0.8746949393526915\n",
    "400,-1.5019022477483523\n",
    "480,-2.185375561680841\n",
    "560,-3.0123437340031307\n",
    "640,2.3513283485530456\n",
    "\"\"\"\n",
    "get_heading_func = create_interpolation_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "braidz_path = \"/gpfs/soma_fs/nfc/nfc3008/Experiments/\"\n",
    "braidz_files = glob.glob(os.path.join(braidz_path, \"*.braidz\"))\n",
    "\n",
    "slp_path = \"/gpfs/soma_fs/home/buchsbaum/sleap_projects/highspeed/predictions/\"\n",
    "slp_folders = glob.glob(os.path.join(slp_path, \"*\",))\n",
    "\n",
    "output_path = \"/gpfs/soma_fs/home/buchsbaum/src/sleap_video_analysis/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e01065f4c1a40a38ef167c50df00be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250421_174810.braidz to data/20250421_174810.csv\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250120_173635.braidz\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250125_150808.braidz\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250430_155345.braidz to data/20250430_155345.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241116_154109.braidz to data/20241116_154109.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241112_124059.braidz to data/20241112_124059.csv\n",
      "No braidz file found for 20250504_191950, trying to search for another file with date 20250504\n",
      "No braidz file found for 20250504_191950, skipping\n",
      "No braidz file found for 20241202_153526, trying to search for another file with date 20241202\n",
      "No braidz file found for 20241202_153526, skipping\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250122_161817.braidz\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250501_183152.braidz to data/20250501_183152.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241125_132912.braidz to data/20241125_132912.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241114_173118.braidz to data/20241114_173118.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250110_142709.braidz to data/20250110_142709.csv\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250114_235655.braidz\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250428_171938.braidz to data/20250428_171938.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241127_152044.braidz to data/20241127_152044.csv\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250116_154619.braidz\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250124_153008.braidz\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250108_160315.braidz to data/20250108_160315.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241129_144233.braidz to data/20241129_144233.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250429_193010.braidz to data/20250429_193010.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250425_195715.braidz to data/20250425_195715.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241121_151406.braidz to data/20241121_151406.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250422_171703.braidz to data/20250422_171703.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250427_170153.braidz to data/20250427_170153.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241126_171308.braidz to data/20241126_171308.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250410_120718.braidz to data/20250410_120718.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250102_152013.braidz to data/20250102_152013.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250111_151946.braidz to data/20250111_151946.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20241130_161634.braidz to data/20241130_161634.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250411_175919.braidz to data/20250411_175919.csv\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250106_153631.braidz to data/20250106_153631.csv\n",
      "No braidz file found for 20250505_191951, trying to search for another file with date 20250505\n",
      "No braidz file found for 20250505_191951, skipping\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250118_165152.braidz\n",
      "No stim or opto data found in /gpfs/soma_fs/nfc/nfc3008/Experiments/20250502_171825.braidz\n",
      "No braidz file found for 20250501_152448, trying to search for another file with date 20250501\n",
      "Interpolated /gpfs/soma_fs/nfc/nfc3008/Experiments/20250501_183152.braidz to data/20250501_183152.csv\n"
     ]
    }
   ],
   "source": [
    "# Loop over slp files and find the corresponding braidz files\n",
    "for slp_folder in tqdm(slp_folders):\n",
    "    filename = pathlib.Path(slp_folder).stem\n",
    "    date = filename.split(\"_\")[0]\n",
    "\n",
    "    braidz_file_to_search = os.path.join(braidz_path, filename + \".braidz\")\n",
    "\n",
    "    # check if braidz file exists in `braidz_files` list\n",
    "    braidz_file = [f for f in braidz_files if filename in f]\n",
    "    braidz_file = braidz_file[0] if braidz_file else None\n",
    "    if braidz_file is None:\n",
    "        print(f\"No braidz file found for {filename}, trying to search for another file with date {date}\")\n",
    "\n",
    "        # if braidz file not found, search for braidz files with the same date but a different time\n",
    "        braidz_file = [f for f in braidz_files if date in f]\n",
    "        braidz_file = braidz_file[0] if braidz_file else None\n",
    "\n",
    "        if braidz_file is None:\n",
    "            print(f\"No braidz file found for {filename}, skipping\")\n",
    "            continue\n",
    "\n",
    "    # make sure `braidz_file` exists\n",
    "    if not os.path.exists(braidz_file):\n",
    "        print(f\"Braidz file {braidz_file} does not exist, skipping\")\n",
    "        continue\n",
    "\n",
    "    # extract stimulus information from braidz file\n",
    "    process_braidz_file(braidz_file, \"data\", get_heading_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_heading_difference(a1, a2):\n",
    "    # Calculate the angular difference considering the circular nature\n",
    "    diff = a1 - a2\n",
    "\n",
    "    # Normalize to [-π, π] range\n",
    "    return np.arctan2(np.sin(diff), np.cos(diff))\n",
    "\n",
    "\n",
    "def sg_smooth(array, window_length=51, polyorder=3, **kwargs):\n",
    "    return savgol_filter(\n",
    "        array, window_length=window_length, polyorder=polyorder, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def unwrap_with_nan(array):\n",
    "    array[~np.isnan(array)] = np.unwrap(array[~np.isnan(array)])\n",
    "    return array\n",
    "\n",
    "\n",
    "def detect_tracking_gaps(df, min_tracked_frames=10, min_gap_size=20):\n",
    "    \"\"\"\n",
    "    Detects if there are multiple tracking sections separated by NaN gaps in the data.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe with tracking data (complete_df)\n",
    "        min_tracked_frames (int): Minimum consecutive frames to consider a valid tracking section\n",
    "        min_gap_size (int): Minimum size of NaN gap to alert about\n",
    "\n",
    "    Returns:\n",
    "        bool: True if multiple tracking sections with gaps are detected\n",
    "    \"\"\"\n",
    "    # Create a mask for rows where all tracking points are valid\n",
    "    valid_mask = (\n",
    "        ~pd.isna(df[\"head.x\"])\n",
    "        & ~pd.isna(df[\"head.y\"])\n",
    "        & ~pd.isna(df[\"abdomen.x\"])\n",
    "        & ~pd.isna(df[\"abdomen.y\"])\n",
    "    )\n",
    "\n",
    "    # Convert mask to integers (1 for valid, 0 for NaN)\n",
    "    valid_series = valid_mask.astype(int)\n",
    "\n",
    "    # Detect changes in the mask (0->1 or 1->0)\n",
    "    # This creates a series where 1 indicates the start or end of a tracking section\n",
    "    changes = valid_series.diff().abs()\n",
    "\n",
    "    # Get indices where changes occur\n",
    "    change_indices = np.where(changes == 1)[0]\n",
    "\n",
    "    # If less than 2 changes, there's only one section or no valid sections\n",
    "    if len(change_indices) < 2:\n",
    "        return False\n",
    "\n",
    "    # Calculate segments\n",
    "    segments = []\n",
    "\n",
    "    # If the first frame is valid, the first change is the end of a segment\n",
    "    start_idx = 0 if valid_series.iloc[0] == 1 else change_indices[0]\n",
    "\n",
    "    for i in range(1 if valid_series.iloc[0] == 1 else 2, len(change_indices), 2):\n",
    "        if i >= len(change_indices):\n",
    "            # If we have an odd number of changes and started with a valid segment\n",
    "            end_idx = len(valid_series) - 1\n",
    "        else:\n",
    "            end_idx = change_indices[i] - 1\n",
    "\n",
    "        # Only include segments that are long enough\n",
    "        segment_length = end_idx - start_idx + 1\n",
    "        if segment_length >= min_tracked_frames:\n",
    "            segments.append((start_idx, end_idx, segment_length))\n",
    "\n",
    "        # Set up for next segment if there are more changes\n",
    "        if i + 1 < len(change_indices):\n",
    "            start_idx = change_indices[i + 1]\n",
    "\n",
    "    # If we have only one valid segment, no need to alert\n",
    "    if len(segments) <= 1:\n",
    "        return False\n",
    "\n",
    "    # Check gaps between segments\n",
    "    for i in range(len(segments) - 1):\n",
    "        current_end = segments[i][1]\n",
    "        next_start = segments[i + 1][0]\n",
    "        gap_size = next_start - current_end - 1\n",
    "\n",
    "        if gap_size >= min_gap_size:\n",
    "            # print(f\"ALERT: Multiple tracking sections detected!\")\n",
    "            # print(f\"  Section 1: Frames {segments[i][0]}-{segments[i][1]} ({segments[i][2]} frames)\")\n",
    "            # print(f\"  Gap: {gap_size} frames with NaNs\")\n",
    "            # print(f\"  Section 2: Frames {segments[i+1][0]}-{segments[i+1][1]} ({segments[i+1][2]} frames)\")\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def savgol_filter_with_nans(y, window_length, polyorder, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply savgol_filter to an array that contains NaNs.\n",
    "    The filter is only applied to contiguous segments of non-NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array_like\n",
    "        The data to be filtered\n",
    "    window_length : int\n",
    "        The length of the filter window (must be odd)\n",
    "    polyorder : int\n",
    "        The order of the polynomial used to fit the samples\n",
    "    **kwargs : dict\n",
    "        Additional arguments to pass to savgol_filter\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    y_filtered : ndarray\n",
    "        The filtered data with NaNs preserved in their original locations\n",
    "    \"\"\"\n",
    "    # Create a copy of the input array to avoid modifying the original\n",
    "    y_filtered = np.copy(y)\n",
    "\n",
    "    # Find indices of non-NaN values\n",
    "    valid_indices = ~np.isnan(y)\n",
    "\n",
    "    if not np.any(valid_indices):\n",
    "        return y_filtered  # Return original if all values are NaN\n",
    "\n",
    "    # Find contiguous segments of valid data\n",
    "    diff_indices = np.diff(np.concatenate(([0], valid_indices.astype(int), [0])))\n",
    "    start_indices = np.where(diff_indices == 1)[0]\n",
    "    end_indices = np.where(diff_indices == -1)[0]\n",
    "    segments = zip(start_indices, end_indices)\n",
    "\n",
    "    # Apply savgol_filter to each segment separately\n",
    "    for start, end in segments:\n",
    "        # Only apply filter if the segment is long enough\n",
    "        if end - start >= window_length:\n",
    "            y_filtered[start:end] = savgol_filter(\n",
    "                y[start:end], window_length, polyorder, **kwargs\n",
    "            )\n",
    "        # Leave shorter segments unfiltered\n",
    "\n",
    "    return y_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(stim_csvs_folder, pre_range=[0, 400], post_range=[400, 750]):\n",
    "    \"\"\"\n",
    "    This function accepts a folder with all the csv files that contain the stimulus data as\n",
    "    extracted from the braid recording.\n",
    "    Then, for each file, it finds the correct folder with the converted slp files, and\n",
    "    inside that folder finds the correct file that matches each row (obj_id + frame) in the stim csv file.\n",
    "\n",
    "    It then loads the data from that file, and calculates the heading difference between pre and post\n",
    "    stimulus data, as well as the heading difference between pre stimulus and the stimulus heading.\n",
    "    The results are returned as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        stim_csvs_folder (str): The folder containing the stimulus CSV files.\n",
    "        pre_range (list): The range of frames to consider for pre-stimulus data.\n",
    "        post_range (list): The range of frames to consider for post-stimulus data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the processed data with heading differences.\n",
    "    \"\"\"\n",
    "    # Create an empty list to collect all the data\n",
    "    all_data = []\n",
    "\n",
    "    # get all csv files in the stim_csvs_folder (these are the stim files)\n",
    "    stim_csvs = sorted(glob.glob(os.path.join(stim_csvs_folder, \"*.csv\")))\n",
    "\n",
    "    # define pattern recognition for filenames\n",
    "    pattern = r\"obj_id_(\\d+)_frame_(\\d+)\"\n",
    "\n",
    "    # loop over all files\n",
    "    for stim_csv in stim_csvs:\n",
    "        print(f\"==== Processing {stim_csv} ====\")\n",
    "        stim_df = pd.read_csv(stim_csv)  # read the csv\n",
    "\n",
    "        # now get the correct folder for the stim file\n",
    "        slp2csv_folder = os.path.join(\n",
    "            stim_csvs_folder,\n",
    "            os.path.join(\n",
    "                os.path.basename(os.path.normpath(stim_csv)).replace(\".csv\", \"\")\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # and get all the files from that folder\n",
    "        slp2csv_files = sorted(glob.glob(os.path.join(slp2csv_folder, \"*.csv\")))\n",
    "\n",
    "        # loop over the rows of each stim file\n",
    "        for idx, row in stim_df.iterrows():\n",
    "            # extract data for each stim row\n",
    "            stim_obj_id = int(row[\"obj_id\"])\n",
    "            stim_frame = int(row[\"frame\"])\n",
    "            stim_heading = float(row[\"stim_heading\"])\n",
    "\n",
    "            # Find the matching csv file\n",
    "            matching_file = None\n",
    "            for file in slp2csv_files:\n",
    "                match = re.search(pattern, file)\n",
    "                if match:\n",
    "                    file_obj_id = int(match.group(1))\n",
    "                    file_frame = int(match.group(2))\n",
    "\n",
    "                    if file_obj_id == stim_obj_id and file_frame == stim_frame:\n",
    "                        matching_file = file\n",
    "                        break\n",
    "\n",
    "            # if no matching file was found, skip\n",
    "            if matching_file is None:\n",
    "                continue\n",
    "\n",
    "            # Load the matching file\n",
    "            data_df = pd.read_csv(matching_file)\n",
    "\n",
    "            # Check if the original data has too few tracked frames\n",
    "            if len(data_df) < 51:\n",
    "                # print(f\"Skipping file with insufficient data: {matching_file}\")\n",
    "                continue\n",
    "\n",
    "            # Create an empty DataFrame with the same structure as data_df\n",
    "            complete_df = pd.DataFrame(columns=data_df.columns)\n",
    "\n",
    "            # Set dtypes to match the original dataframe\n",
    "            for col in data_df.columns:\n",
    "                complete_df[col] = complete_df[col].astype(data_df[col].dtype)\n",
    "\n",
    "            # Fill the frame_idx column with all possible frames (0-749)\n",
    "            complete_df[\"frame_idx\"] = np.array(range(750))\n",
    "\n",
    "            # Set the index to frame_idx for easier merging\n",
    "            complete_df = complete_df.set_index(\"frame_idx\")\n",
    "            data_df_indexed = data_df.set_index(\"frame_idx\")\n",
    "\n",
    "            # Update the complete_df with values from the original data_df\n",
    "            complete_df.update(data_df_indexed)\n",
    "\n",
    "            # Reset index to make frame_idx a column again\n",
    "            complete_df = complete_df.reset_index()\n",
    "\n",
    "            # Example usage in your code:\n",
    "            if detect_tracking_gaps(\n",
    "                complete_df, min_tracked_frames=10, min_gap_size=20\n",
    "            ):\n",
    "                has_tracking_gaps = True\n",
    "\n",
    "            # Now interpolate to fill the gaps in tracking data\n",
    "            data_df_interp = complete_df.interpolate(\n",
    "                method=\"linear\", limit_direction=\"both\", limit=25\n",
    "            )\n",
    "\n",
    "            # extract all data and apply smoothing\n",
    "            frames = data_df_interp[\"frame_idx\"].to_numpy()\n",
    "            head_x = savgol_filter_with_nans(\n",
    "                data_df_interp[\"head.x\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "            head_y = savgol_filter_with_nans(\n",
    "                data_df_interp[\"head.y\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "            abdomen_x = savgol_filter_with_nans(\n",
    "                data_df_interp[\"abdomen.x\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "            abdomen_y = savgol_filter_with_nans(\n",
    "                data_df_interp[\"abdomen.y\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "\n",
    "            # head_x = sg_smooth(data_df_interp[\"head.x\"].to_numpy())\n",
    "            # head_y = sg_smooth(data_df_interp[\"head.y\"].to_numpy())\n",
    "            # abdomen_x = sg_smooth(data_df_interp[\"abdomen.x\"].to_numpy())\n",
    "            # abdomen_y = sg_smooth(data_df_interp[\"abdomen.y\"].to_numpy())\n",
    "\n",
    "            # extract all frames in pre and post stimulus ranges\n",
    "            pre_indices = np.where((frames >= pre_range[0]) & (frames < pre_range[1]))[\n",
    "                0\n",
    "            ]\n",
    "            post_indices = np.where(\n",
    "                (frames >= post_range[0]) & (frames < post_range[1])\n",
    "            )[0]\n",
    "\n",
    "            # Count frames with valid (non-NaN) tracking data in pre-range\n",
    "            pre_valid_mask = (\n",
    "                ~np.isnan(head_x[pre_indices])\n",
    "                & ~np.isnan(head_y[pre_indices])\n",
    "                & ~np.isnan(abdomen_x[pre_indices])\n",
    "                & ~np.isnan(abdomen_y[pre_indices])\n",
    "            )\n",
    "            pre_valid_count = np.sum(pre_valid_mask)\n",
    "\n",
    "            # Count frames with valid (non-NaN) tracking data in post-range\n",
    "            post_valid_mask = (\n",
    "                ~np.isnan(head_x[post_indices])\n",
    "                & ~np.isnan(head_y[post_indices])\n",
    "                & ~np.isnan(abdomen_x[post_indices])\n",
    "                & ~np.isnan(abdomen_y[post_indices])\n",
    "            )\n",
    "            post_valid_count = np.sum(post_valid_mask)\n",
    "\n",
    "            # Skip if not enough valid frames in these ranges\n",
    "            if pre_valid_count < 10 or post_valid_count < 10:\n",
    "                continue\n",
    "\n",
    "            # Keep only valid indices\n",
    "            pre_indices = pre_indices[pre_valid_mask]\n",
    "            post_indices = post_indices[post_valid_mask]\n",
    "\n",
    "            # extract pre-stimulus coordinates\n",
    "            head_x_pre = head_x[pre_indices]\n",
    "            head_y_pre = head_y[pre_indices]\n",
    "            abdomen_x_pre = abdomen_x[pre_indices]\n",
    "            abdomen_y_pre = abdomen_y[pre_indices]\n",
    "\n",
    "            # extract post-stimulus coordinates\n",
    "            head_x_post = head_x[post_indices]\n",
    "            head_y_post = head_y[post_indices]\n",
    "            abdomen_x_post = abdomen_x[post_indices]\n",
    "            abdomen_y_post = abdomen_y[post_indices]\n",
    "\n",
    "            # calculate heading for each frame (angle of vector from abdomen to head)\n",
    "            pre_heading = np.arctan2(\n",
    "                head_y_pre - abdomen_y_pre, head_x_pre - abdomen_x_pre\n",
    "            )\n",
    "            post_heading = np.arctan2(\n",
    "                head_y_post - abdomen_y_post, head_x_post - abdomen_x_post\n",
    "            )\n",
    "\n",
    "            # calculate circular mean of headings (accounts for circular nature of angle data)\n",
    "            pre_heading_mean = circmean(pre_heading, high=np.pi, low=-np.pi)\n",
    "            post_heading_mean = circmean(post_heading, high=np.pi, low=-np.pi)\n",
    "\n",
    "            # calculate heading differences\n",
    "            try:\n",
    "                # Calculate the difference between post-stimulus and pre-stimulus headings\n",
    "                prepost_heading_difference = calculate_heading_difference(\n",
    "                    post_heading_mean, pre_heading_mean\n",
    "                )\n",
    "\n",
    "                # Calculate the difference between stimulus heading and pre-stimulus heading\n",
    "                prestim_heading_difference = calculate_heading_difference(\n",
    "                    stim_heading, pre_heading_mean\n",
    "                )\n",
    "\n",
    "                # Calculate the difference between stimulus heading and post-stimulus heading\n",
    "                poststim_heading_difference = calculate_heading_difference(\n",
    "                    stim_heading, post_heading_mean\n",
    "                )\n",
    "\n",
    "                # Determine if the fly turned toward or away from the stimulus\n",
    "                # Get sign of stimulus position relative to fly\n",
    "                stimulus_direction = np.sign(prestim_heading_difference)\n",
    "\n",
    "                # Get sign of the turn\n",
    "                turn_direction = np.sign(prepost_heading_difference)\n",
    "\n",
    "                # If stimulus_direction and turn_direction have opposite signs,\n",
    "                # the fly turned away from the stimulus\n",
    "                turned_away = stimulus_direction * turn_direction < 0\n",
    "\n",
    "                # Create a copy of the row data and add the new calculations\n",
    "                row_data = row.to_dict()  # Convert the row to a dictionary\n",
    "                row_data[\"prepost_heading_difference\"] = prepost_heading_difference\n",
    "                row_data[\"prestim_heading_difference\"] = prestim_heading_difference\n",
    "                row_data[\"poststim_heading_difference\"] = poststim_heading_difference\n",
    "                row_data[\"pre_heading\"] = pre_heading_mean\n",
    "                row_data[\"post_heading\"] = post_heading_mean\n",
    "                row_data[\"turned_away\"] = turned_away\n",
    "                row_data[\"turn_direction\"] = \"Away\" if turned_away else \"Toward\"\n",
    "\n",
    "                # Append to the all_data list\n",
    "                all_data.append(row_data)\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Error calculating heading difference: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Create a DataFrame from all the collected data\n",
    "    result_df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Now result_df contains all the data from all files with the heading differences\n",
    "    print(f\"Combined DataFrame has {len(result_df)} rows\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_results_df = process_data(\n",
    "    \"/gpfs/soma_fs/home/buchsbaum/src/sleap_video_analysis/output/canton-s/\"\n",
    ")\n",
    "native_results_df = process_data(\n",
    "    \"/gpfs/soma_fs/home/buchsbaum/src/sleap_video_analysis/output/native/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin data by `prestim_heading_difference`\n",
    "# where -45 to +45 is `front`, -45 to -135 is `left`, +45 to +135 is `right`, rest is 'back'\n",
    "def bin_heading_difference(row):\n",
    "    if (\n",
    "        row[\"prestim_heading_difference\"] >= -np.pi / 4\n",
    "        and row[\"prestim_heading_difference\"] <= np.pi / 4\n",
    "    ):\n",
    "        return \"front\"\n",
    "    elif (\n",
    "        row[\"prestim_heading_difference\"] > np.pi / 4\n",
    "        and row[\"prestim_heading_difference\"] <= 3 * np.pi / 4\n",
    "    ):\n",
    "        return \"right\"\n",
    "    elif (\n",
    "        row[\"prestim_heading_difference\"] < -np.pi / 4\n",
    "        and row[\"prestim_heading_difference\"] >= -3 * np.pi / 4\n",
    "    ):\n",
    "        return \"left\"\n",
    "    else:\n",
    "        return \"back\"\n",
    "\n",
    "\n",
    "# Apply the function to create a new column\n",
    "cs_results_df[\"prestim_heading_bin\"] = cs_results_df.apply(\n",
    "    bin_heading_difference, axis=1\n",
    ")\n",
    "native_results_df[\"prestim_heading_bin\"] = native_results_df.apply(\n",
    "    bin_heading_difference, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_for_all_bins(results_df):\n",
    "    for bin in results_df[\"prestim_heading_bin\"].unique():\n",
    "        fig, ax = plt.subplots()\n",
    "        bin_df = results_df[results_df[\"prestim_heading_bin\"] == bin]\n",
    "        sns.histplot(\n",
    "            data=bin_df,\n",
    "            x=\"prepost_heading_difference\",\n",
    "            bins=np.linspace(-np.pi, np.pi, 30),\n",
    "            common_bins=True,\n",
    "            stat=\"density\",\n",
    "            label=bin,\n",
    "            ax=ax,\n",
    "            common_norm=True,\n",
    "            kde=True,\n",
    "        )\n",
    "\n",
    "        # add line for median and mean\n",
    "        mean = circmean(bin_df[\"prepost_heading_difference\"], high=np.pi, low=-np.pi)\n",
    "\n",
    "        ax.axvline(mean, color=\"g\", linestyle=\"--\", label=\"Mean\")\n",
    "        ax.set_title(f\"Heading Difference Histogram - {bin}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_histogram_for_all_bins(cs_results_df)\n",
    "plot_histogram_for_all_bins(native_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_clustering(stim_csvs_folder, window=0):\n",
    "    # Create a list to store all feature matrices\n",
    "    all_features = []\n",
    "    # Create a list to store metadata\n",
    "    all_metadata = []\n",
    "\n",
    "    # get all csv files in the stim_csvs_folder (these are the stim files)\n",
    "    stim_csvs = sorted(glob.glob(os.path.join(stim_csvs_folder, \"*.csv\")))\n",
    "\n",
    "    # define pattern recognition for filenames\n",
    "    pattern = r\"obj_id_(\\d+)_frame_(\\d+)\"\n",
    "\n",
    "    # Inside your nested loops, after extracting features:\n",
    "    for i, stim_csv in enumerate(stim_csvs):\n",
    "        print(f\"Processing file {stim_csv} ({i} out of {len(stim_csvs)})\")\n",
    "        # print(f\"==== Processing {stim_csv} ====\")\n",
    "        stim_df = pd.read_csv(stim_csv)  # read the csv\n",
    "\n",
    "        # now get the correct folder for the stim file\n",
    "        slp2csv_folder = os.path.join(\n",
    "            stim_csvs_folder,\n",
    "            os.path.join(\n",
    "                os.path.basename(os.path.normpath(stim_csv)).replace(\".csv\", \"\")\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # and get all the files from that folder\n",
    "        slp2csv_files = sorted(glob.glob(os.path.join(slp2csv_folder, \"*.csv\")))\n",
    "\n",
    "        for j, (idx, row) in enumerate(stim_df.iterrows()):\n",
    "            # extract data for each stim row\n",
    "            stim_obj_id = int(row[\"obj_id\"])\n",
    "            stim_frame = int(row[\"frame\"])\n",
    "            stim_heading = float(row[\"stim_heading\"])\n",
    "\n",
    "            # Find the matching csv file\n",
    "            matching_file = None\n",
    "            for file in slp2csv_files:\n",
    "                match = re.search(pattern, file)\n",
    "                if match:\n",
    "                    file_obj_id = int(match.group(1))\n",
    "                    file_frame = int(match.group(2))\n",
    "\n",
    "                    if file_obj_id == stim_obj_id and file_frame == stim_frame:\n",
    "                        matching_file = file\n",
    "                        break\n",
    "\n",
    "            # if no matching file was found, skip\n",
    "            if matching_file is None:\n",
    "                continue\n",
    "\n",
    "            # Load the matching file\n",
    "            data_df = pd.read_csv(matching_file)\n",
    "\n",
    "            # Create an empty DataFrame with the same structure as data_df\n",
    "            complete_df = pd.DataFrame(columns=data_df.columns)\n",
    "\n",
    "            # Set dtypes to match the original dataframe\n",
    "            for col in data_df.columns:\n",
    "                complete_df[col] = complete_df[col].astype(data_df[col].dtype)\n",
    "\n",
    "            # Fill the frame_idx column with all possible frames (0-749)\n",
    "            complete_df[\"frame_idx\"] = list(range(750))\n",
    "\n",
    "            # Set the index to frame_idx for easier merging\n",
    "            complete_df = complete_df.set_index(\"frame_idx\")\n",
    "            data_df_indexed = data_df.set_index(\"frame_idx\")\n",
    "\n",
    "            # Update the complete_df with values from the original data_df\n",
    "            complete_df.update(data_df_indexed)\n",
    "\n",
    "            # Reset index to make frame_idx a column again\n",
    "            complete_df = complete_df.reset_index()\n",
    "\n",
    "            # Example usage in your code:\n",
    "\n",
    "            # Now interpolate to fill the gaps in tracking data\n",
    "            data_df_interp = complete_df.interpolate(\n",
    "                method=\"linear\", limit_direction=\"both\", limit=25\n",
    "            )\n",
    "\n",
    "            # extract all data and apply smoothing\n",
    "            frames = data_df_interp[\"frame_idx\"].to_numpy()\n",
    "            head_x = savgol_filter_with_nans(\n",
    "                data_df_interp[\"head.x\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "            head_y = savgol_filter_with_nans(\n",
    "                data_df_interp[\"head.y\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "            abdomen_x = savgol_filter_with_nans(\n",
    "                data_df_interp[\"abdomen.x\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "            abdomen_y = savgol_filter_with_nans(\n",
    "                data_df_interp[\"abdomen.y\"].to_numpy(), window_length=51, polyorder=3\n",
    "            )\n",
    "\n",
    "            # Calculate different features\n",
    "            heading = np.arctan2(head_y - abdomen_y, head_x - abdomen_x)\n",
    "            heading_unwrap = unwrap_with_nan(heading)\n",
    "            heading_change = savgol_filter(\n",
    "                np.gradient(heading_unwrap, 1 / 500), window_length=21, polyorder=3\n",
    "            )\n",
    "            distance_between_head_and_abdomen = np.sqrt(\n",
    "                (head_x - abdomen_x) ** 2 + (head_y - abdomen_y) ** 2\n",
    "            )\n",
    "\n",
    "            # calculate centroid based on head and abodomen coordinates\n",
    "            centroid_x = (head_x + abdomen_x) / 2\n",
    "            centroid_y = (head_y + abdomen_y) / 2\n",
    "            centroid_velocity_x = savgol_filter(\n",
    "                np.gradient(centroid_x, 1 / 500), window_length=21, polyorder=3\n",
    "            )\n",
    "            centroid_velocity_y = savgol_filter(\n",
    "                np.gradient(centroid_y, 1 / 500), window_length=21, polyorder=3\n",
    "            )\n",
    "            centroid_velocity = np.sqrt(centroid_velocity_x**2 + centroid_velocity_y**2)\n",
    "            centroid_acceleration = savgol_filter(\n",
    "                np.gradient(centroid_velocity, 1 / 500), window_length=21, polyorder=3\n",
    "            )\n",
    "\n",
    "            theta = np.arctan2(centroid_velocity_y, centroid_velocity_x)\n",
    "            theta_unwrap = unwrap_with_nan(theta)\n",
    "            angular_velocity = savgol_filter(\n",
    "                np.gradient(theta_unwrap, 1 / 500), window_length=21, polyorder=3\n",
    "            )\n",
    "\n",
    "            # find peaks in the angular velocity\n",
    "            positive_peaks, _ = find_peaks(\n",
    "                angular_velocity, height=np.deg2rad(1000), distance=50\n",
    "            )\n",
    "            negative_peaks, _ = find_peaks(\n",
    "                -angular_velocity, height=np.deg2rad(1000), distance=50\n",
    "            )\n",
    "            all_peaks = np.sort(np.concatenate((positive_peaks, negative_peaks)))\n",
    "\n",
    "            # find if there are any peaks in range\n",
    "            response_peak = [peak for peak in all_peaks if 350 < peak < 450]\n",
    "\n",
    "            if len(response_peak) > 0:\n",
    "                response_peak = response_peak[0]\n",
    "\n",
    "                if window == 0:\n",
    "                    indices = response_peak\n",
    "                else:\n",
    "                    indices = range(response_peak - window, response_peak + window)\n",
    "\n",
    "                heading = heading[indices]\n",
    "\n",
    "                if np.any(np.isnan(heading)):\n",
    "                    continue\n",
    "\n",
    "                heading_change = heading_change[indices]\n",
    "                distance_between_head_and_abdomen = distance_between_head_and_abdomen[\n",
    "                    indices\n",
    "                ]\n",
    "                centroid_velocity = centroid_velocity[indices]\n",
    "                centroid_acceleration = centroid_acceleration[indices]\n",
    "                angular_velocity = angular_velocity[indices]\n",
    "\n",
    "                frames = frames[indices]\n",
    "                x = centroid_x[indices]\n",
    "                y = centroid_y[indices]\n",
    "\n",
    "                temp_dict = {\n",
    "                    \"file\": np.array([i] * len(indices)),\n",
    "                    \"time\": frames,\n",
    "                    \"x\": x,\n",
    "                    \"y\": y,\n",
    "                    \"heading\": heading,\n",
    "                    \"heading_change\": heading_change,\n",
    "                    \"distance_between_head_and_abdomen\": distance_between_head_and_abdomen,\n",
    "                    \"centroid_velocity\": centroid_velocity,\n",
    "                    \"centroid_acceleration\": centroid_acceleration,\n",
    "                    \"angular_velocity\": angular_velocity,\n",
    "                }\n",
    "\n",
    "                temp_df = pd.DataFrame(temp_dict)\n",
    "                all_features.append(temp_df)\n",
    "    return pd.concat(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_features = get_data_for_clustering(\n",
    "    \"/gpfs/soma_fs/home/buchsbaum/src/sleap_video_analysis/canton-s/\", window=25\n",
    ")\n",
    "native_features = get_data_for_clustering(\n",
    "    \"/gpfs/soma_fs/home/buchsbaum/src/sleap_video_analysis/native/\", window=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_features.reset_index(drop=True, inplace=True)\n",
    "native_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cs_features.interpolate(inplace=True)\n",
    "native_features.interpolate(inplace=True)\n",
    "\n",
    "# print number of nan values in each column\n",
    "print(\"CS features:\")\n",
    "print(cs_features.isna().sum())\n",
    "print(\"Native features:\")\n",
    "print(native_features.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, check for and handle zero-variance features\n",
    "def standardize_with_checks(df, columns_to_transform):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_std = df.copy()\n",
    "\n",
    "    # Check for zero variance columns\n",
    "    for col in df.iloc[:, columns_to_transform].columns:\n",
    "        if df[col].std() < 1e-10:\n",
    "            print(f\"Warning: Column {col} has near-zero variance\")\n",
    "            # Add tiny amount of noise to prevent zero variance\n",
    "            df_std[col] += np.random.normal(0, 1e-5, size=len(df))\n",
    "\n",
    "    # Apply standardization\n",
    "    scaler = StandardScaler()\n",
    "    df_std.iloc[:, columns_to_transform] = scaler.fit_transform(\n",
    "        df_std.iloc[:, columns_to_transform]\n",
    "    )\n",
    "\n",
    "    return df_std, scaler\n",
    "\n",
    "\n",
    "# 2. Modify your UMAP function to handle special metrics properly\n",
    "def draw_umap_safe(\n",
    "    data,\n",
    "    features,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,\n",
    "    metric=\"euclidean\",\n",
    "    title=\"\",\n",
    "):\n",
    "    # Get the data matrix\n",
    "    X = data.iloc[:, features].to_numpy()\n",
    "\n",
    "    # Set up metric-specific parameters\n",
    "    metric_params = {}\n",
    "\n",
    "    if metric == \"mahalanobis\":\n",
    "        # Calculate covariance matrix with regularization\n",
    "        cov = np.cov(X, rowvar=False)\n",
    "        # Add small regularization to ensure positive definiteness\n",
    "        cov += np.eye(cov.shape[0]) * 1e-6\n",
    "        try:\n",
    "            inv_cov = np.linalg.inv(cov)\n",
    "            metric_params = {\"V\": inv_cov}\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\n",
    "                \"Warning: Covariance matrix is singular, using stronger regularization\"\n",
    "            )\n",
    "            cov += np.eye(cov.shape[0]) * 1e-4\n",
    "            inv_cov = np.linalg.inv(cov)\n",
    "            metric_params = {\"V\": inv_cov}\n",
    "\n",
    "    elif metric == \"seuclidean\":\n",
    "        # Calculate variance for each dimension, avoiding zeros\n",
    "        variances = np.var(X, axis=0, ddof=1)\n",
    "        # Ensure no zeros in variances\n",
    "        variances = np.maximum(variances, 1e-8)\n",
    "        metric_params = {\"V\": variances}\n",
    "\n",
    "    # Create UMAP with proper parameters\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric,\n",
    "        metric_kwds=metric_params if metric_params else {},\n",
    "        random_state=42,  # For reproducibility\n",
    "    )\n",
    "\n",
    "    u = fit.fit_transform(X)\n",
    "\n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    if n_components == 1:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:, 0], range(len(u)))\n",
    "    if n_components == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        scatter = ax.scatter(u[:, 0], u[:, 1], c=data[\"file\"], alpha=0.7)\n",
    "        plt.colorbar(scatter, label=\"File\")\n",
    "    if n_components == 3:\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        scatter = ax.scatter(u[:, 0], u[:, 1], u[:, 2], c=data[\"file\"], alpha=0.7)\n",
    "        plt.colorbar(scatter, label=\"File\")\n",
    "\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fit, u\n",
    "\n",
    "\n",
    "# 3. Use these functions in your main code\n",
    "columns_to_transform = [4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Standardize data with checks\n",
    "cs_features_std, scaler = standardize_with_checks(cs_features, columns_to_transform)\n",
    "\n",
    "# Now run your experiments with safer implementation\n",
    "neighbours = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "min_dists = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "metrics = [\"euclidean\", \"mahalanobis\", \"correlation\", \"cosine\"]\n",
    "\n",
    "pbar = tqdm(total=len(neighbours) * len(min_dists) * len(metrics))\n",
    "for n in neighbours:\n",
    "    for m in min_dists:\n",
    "        for met in metrics:\n",
    "            try:\n",
    "                fit, u = draw_umap_safe(\n",
    "                    cs_features_std,\n",
    "                    features=columns_to_transform,\n",
    "                    n_neighbors=n,\n",
    "                    min_dist=m,\n",
    "                    metric=met,\n",
    "                    title=f\"CS: n_neighbors={n}, min_dist={m}, metric={met}\",\n",
    "                )\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {met} metric: {str(e)}\")\n",
    "            finally:\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 1. First, check for and handle zero-variance features\n",
    "def standardize_with_checks(df, columns_to_transform):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_std = df.copy()\n",
    "\n",
    "    # Check for zero variance columns\n",
    "    for col in df.iloc[:, columns_to_transform].columns:\n",
    "        if df[col].std() < 1e-10:\n",
    "            print(f\"Warning: Column {col} has near-zero variance\")\n",
    "            # Add tiny amount of noise to prevent zero variance\n",
    "            df_std[col] += np.random.normal(0, 1e-5, size=len(df))\n",
    "\n",
    "    # Apply standardization\n",
    "    scaler = StandardScaler()\n",
    "    df_std.iloc[:, columns_to_transform] = scaler.fit_transform(\n",
    "        df_std.iloc[:, columns_to_transform]\n",
    "    )\n",
    "\n",
    "    return df_std, scaler\n",
    "\n",
    "\n",
    "# 2. Define a function to compute UMAP embedding in parallel (no plotting)\n",
    "def compute_umap(n, m, met, data, features, n_components=2):\n",
    "    try:\n",
    "        # Get the data matrix\n",
    "        X = data.iloc[:, features].to_numpy()\n",
    "\n",
    "        # Set up metric-specific parameters\n",
    "        metric_params = {}\n",
    "\n",
    "        if met == \"mahalanobis\":\n",
    "            # Calculate covariance matrix with regularization\n",
    "            cov = np.cov(X, rowvar=False)\n",
    "            # Add small regularization to ensure positive definiteness\n",
    "            cov += np.eye(cov.shape[0]) * 1e-6\n",
    "            try:\n",
    "                inv_cov = np.linalg.inv(cov)\n",
    "                metric_params = {\"V\": inv_cov}\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\n",
    "                    \"Warning: Covariance matrix is singular, using stronger regularization\"\n",
    "                )\n",
    "                cov += np.eye(cov.shape[0]) * 1e-4\n",
    "                inv_cov = np.linalg.inv(cov)\n",
    "                metric_params = {\"V\": inv_cov}\n",
    "\n",
    "        elif met == \"seuclidean\":\n",
    "            # Calculate variance for each dimension, avoiding zeros\n",
    "            variances = np.var(X, axis=0, ddof=1)\n",
    "            # Ensure no zeros in variances\n",
    "            variances = np.maximum(variances, 1e-8)\n",
    "            metric_params = {\"V\": variances}\n",
    "\n",
    "        # Create UMAP with proper parameters\n",
    "        fit = umap.UMAP(\n",
    "            n_neighbors=n,\n",
    "            min_dist=m,\n",
    "            n_components=n_components,\n",
    "            metric=met,\n",
    "            metric_kwds=metric_params if metric_params else {},\n",
    "            random_state=42,  # For reproducibility\n",
    "        )\n",
    "\n",
    "        u = fit.fit_transform(X)\n",
    "\n",
    "        # Save results to file to avoid memory issues\n",
    "        result_filename = f\"umap_temp/umap_n{n}_m{m}_{met}.npz\"\n",
    "        np.savez(result_filename, embedding=u)\n",
    "\n",
    "        return True, (n, m, met)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with n={n}, min_dist={m}, metric={met}: {str(e)}\")\n",
    "        return False, (n, m, met)\n",
    "\n",
    "\n",
    "# 3. Function to visualize a single UMAP result\n",
    "def visualize_umap(n, m, met, data, features, save_dir=\"umap_plots\", show_plot=False):\n",
    "    try:\n",
    "        # Load saved embedding\n",
    "        result_filename = f\"umap_temp/umap_n{n}_m{m}_{met}.npz\"\n",
    "        if not os.path.exists(result_filename):\n",
    "            return False\n",
    "\n",
    "        loaded = np.load(result_filename)\n",
    "        embedding = loaded[\"embedding\"]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "        # Determine number of components from embedding shape\n",
    "        n_components = embedding.shape[1]\n",
    "\n",
    "        # Plot based on the number of components\n",
    "        if n_components == 1:\n",
    "            plt.scatter(embedding[:, 0], range(len(embedding)))\n",
    "        elif n_components == 2:\n",
    "            scatter = plt.scatter(\n",
    "                embedding[:, 0], embedding[:, 1], c=data[\"file\"], alpha=0.7\n",
    "            )\n",
    "            plt.colorbar(scatter, label=\"File\")\n",
    "        elif n_components == 3:\n",
    "            ax = fig.add_subplot(111, projection=\"3d\")\n",
    "            scatter = ax.scatter(\n",
    "                embedding[:, 0],\n",
    "                embedding[:, 1],\n",
    "                embedding[:, 2],\n",
    "                c=data[\"file\"],\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.colorbar(scatter, label=\"File\")\n",
    "\n",
    "        plt.title(f\"CS: n_neighbors={n}, min_dist={m}, metric={met}\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure\n",
    "        filename = f\"{save_dir}/umap_n{n}_m{m}_{met}.png\"\n",
    "        plt.savefig(filename)\n",
    "\n",
    "        # Show the plot if requested\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing n={n}, min_dist={m}, metric={met}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Standardize data with checks\n",
    "columns_to_transform = [4, 5, 6, 7, 8, 9]\n",
    "cs_features_std, scaler = standardize_with_checks(cs_features, columns_to_transform)\n",
    "\n",
    "# Define parameter ranges\n",
    "neighbours = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "min_dists = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "metrics = [\"euclidean\", \"mahalanobis\", \"correlation\", \"cosine\"]\n",
    "\n",
    "# Create directories for saving plots and temporary results\n",
    "os.makedirs(\"umap_plots\", exist_ok=True)\n",
    "os.makedirs(\"umap_temp\", exist_ok=True)\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_combinations = [\n",
    "    (n, m, met) for n in neighbours for m in min_dists for met in metrics\n",
    "]\n",
    "print(\n",
    "    f\"Running UMAP with {len(param_combinations)} parameter combinations in parallel...\"\n",
    ")\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 10  # Adjust based on your system's capabilities\n",
    "total_batches = (len(param_combinations) + batch_size - 1) // batch_size\n",
    "\n",
    "all_successful = []\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(param_combinations))\n",
    "    current_batch = param_combinations[start_idx:end_idx]\n",
    "\n",
    "    print(\n",
    "        f\"Processing batch {batch_idx + 1}/{total_batches} ({len(current_batch)} combinations)\"\n",
    "    )\n",
    "\n",
    "    # Run UMAP computations in parallel\n",
    "    results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
    "        delayed(compute_umap)(n, m, met, cs_features_std, columns_to_transform)\n",
    "        for n, m, met in current_batch\n",
    "    )\n",
    "\n",
    "    # Track successful results\n",
    "    successful_in_batch = [params for success, params in results if success]\n",
    "    all_successful.extend(successful_in_batch)\n",
    "\n",
    "    # Allow system to recover before next batch\n",
    "    time.sleep(1)\n",
    "\n",
    "# Process results and create visualizations sequentially\n",
    "print(\"Creating visualizations...\")\n",
    "for n, m, met in tqdm(all_successful):\n",
    "    visualize_umap(n, m, met, cs_features_std, columns_to_transform)\n",
    "\n",
    "# Print summary\n",
    "print(\n",
    "    f\"Successfully created {len(all_successful)} UMAP plots out of {len(param_combinations)} combinations.\"\n",
    ")\n",
    "print(\"Plots saved to the 'umap_plots' directory.\")\n",
    "\n",
    "# Optional: clean up temporary files\n",
    "# import shutil\n",
    "# shutil.rmtree(\"umap_temp\")\n",
    "# print(\"Temporary files removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
